{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model / weights, set files & batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 8972\n",
      "Number of Batches: 179\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import absl.logging\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import model\n",
    "\n",
    "\n",
    "def train(test_or_data: str, files: list, restart: str, callbacks: list):\n",
    "    nn = model.create_model(model.metrics)\n",
    "    if restart == \"y\" or \"Y\":\n",
    "        np.save(\"models/mae.npy\", [])\n",
    "        np.save(\"models/precision.npy\", [])\n",
    "    else:\n",
    "        checkpoint = tf.train.latest_checkpoint(\"./models\")\n",
    "        nn.load_weights(checkpoint)\n",
    "\n",
    "    for batch in tqdm(range(len(files) // model.batch_size)):\n",
    "        batch_files = files[batch * model.batch_size : (batch + 1) * model.batch_size]\n",
    "        spectra = []\n",
    "        labels = []\n",
    "        for file in batch_files:\n",
    "            folder = file.split(\"/\")[3]\n",
    "            if folder == f\"{test_or_data}\":\n",
    "                df = pd.read_parquet(file)\n",
    "                spectra.append([a.tolist() for a in df.spectra.values])\n",
    "                labels.append(list(df.labels.values))\n",
    "\n",
    "            elif folder == \"backgrounds\":\n",
    "                with open(file) as f:\n",
    "                    contents = sample(f.readlines(), model.num_spectra)\n",
    "                    for line in contents:\n",
    "                        spectra.append([float(i) for i in line.split(\",\")])\n",
    "                    labels.append([0 for _ in range(13)])\n",
    "\n",
    "        if len(spectra) != len(labels):\n",
    "            continue\n",
    "\n",
    "        spectra = np.array(spectra).reshape(model.batch_size * model.num_spectra, 4563)\n",
    "        labels = np.array(labels).reshape(model.batch_size * model.num_spectra, 13)\n",
    "\n",
    "        nn.fit(spectra, labels, epochs=model.epochs, verbose=False, callbacks=callbacks)\n",
    "\n",
    "\n",
    "class SaveMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        np.save(\"models/mae.npy\", np.append(np.load(\"models/mae.npy\"), logs[\"mae\"]))\n",
    "        np.save(\"models/precision.npy\", np.append(np.load(\"models/precision.npy\"), logs[\"precision\"]))\n",
    "\n",
    "\n",
    "metric_callback = SaveMetricsCallback()\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model.checkpoint_folder,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"mae\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_or_data = \"test_set\"\n",
    "\n",
    "data_files = glob(f\"../1_data_generation/training_data/{test_or_data}/*/*\")\n",
    "background_files = glob(\"../1_data_generation/training_data/backgrounds/*\")\n",
    "\n",
    "# Put data_files and background files together\n",
    "# Double the data file occurences as to get more null sampels\n",
    "if test_or_data == \"test_set\":\n",
    "    files = data_files\n",
    "else:\n",
    "    files = data_files + background_files + background_files\n",
    "\n",
    "shuffle(files)\n",
    "print(f\"Number of Files: {len(files)}\")\n",
    "print(f\"Number of Batches: {len(files)//model.batch_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 6/179 [13:44<6:30:39, 135.49s/it]"
     ]
    }
   ],
   "source": [
    "restart = input(\"Do you want to restart? [y]es or [n]o:\\t\")\n",
    "train(test_or_data, files, restart, [metric_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "135b39839cb5e8387ae30e7e8a2a61075d863fcb332621737cbf0179a601a6b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
